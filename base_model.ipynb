{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression, Lasso, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, log_loss\n",
    "\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_x = './input_data/input_training_IrTAw7w.csv'\n",
    "filename_y = './input_data/output_training_F2dZW38.csv'\n",
    "\n",
    "df = pd.read_csv(filename_x).iloc[:, 1:]\n",
    "Y = pd.read_csv(filename_y).iloc[:, 1:]\n",
    "\n",
    "Y = np.argmax(np.array(Y), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use only X1 - X5 to train a simple logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.23      0.27       673\n",
      "           1       0.34      0.40      0.37       721\n",
      "           2       0.37      0.42      0.39       706\n",
      "\n",
      "    accuracy                           0.35      2100\n",
      "   macro avg       0.35      0.35      0.34      2100\n",
      "weighted avg       0.35      0.35      0.34      2100\n",
      "\n",
      "0.35\n"
     ]
    }
   ],
   "source": [
    "# remove history sentiment data\n",
    "\n",
    "train = df.loc[:, 'X1':\"X5\"]\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(train, Y,  \n",
    "                                                      test_size=0.2)\n",
    "\n",
    "\n",
    "parameter_grid = {\"C\":[0.6, 0.7, 0.8, 0.9, 1]}\n",
    "simple_model = GridSearchCV(LogisticRegression(), param_grid=parameter_grid,\n",
    "                            n_jobs=-1)\n",
    "simple_model.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "print(classification_report(y_valid, simple_model.predict(x_valid)))\n",
    "print(accuracy_score(y_valid, simple_model.predict(x_valid)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use all the sentiment data to train a regression model with l1 reguralization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.07      0.12       695\n",
      "           1       0.36      0.72      0.48       701\n",
      "           2       0.36      0.29      0.32       704\n",
      "\n",
      "    accuracy                           0.36      2100\n",
      "   macro avg       0.37      0.36      0.30      2100\n",
      "weighted avg       0.37      0.36      0.31      2100\n",
      "\n",
      "0.36\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# get all history sentiment data\n",
    "sentiment = df.loc[:, 'I1_lag0':'I10_lag47']\n",
    "train = sentiment\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(train, Y,  \n",
    "                                                      test_size=0.2)\n",
    "\n",
    "\n",
    "parameter_grid = {\"alpha\":[1e-2, 3e-2, 1e-1, 3e-1, 1]}\n",
    "simple_model = GridSearchCV(SGDClassifier(penalty='l1', loss='log'), \n",
    "                            param_grid=parameter_grid,\n",
    "                            n_jobs=-1)\n",
    "simple_model.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "print(classification_report(y_valid, simple_model.predict(x_valid)))\n",
    "print(accuracy_score(y_valid, simple_model.predict(x_valid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use X1-X10, max, min, median, mean, trend of sentiment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-b1887f082ef6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m \u001b[0ma\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;31m# Get the max, min, median, mean and trend of sentiment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[0moverall_std\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"I1_lag0\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m\"I10_lag47\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "def get_trend_of_ts(ts):\n",
    "    '''\n",
    "    get the slope of time series, if the slope is not significant, set it to 0\n",
    "    \n",
    "    input:\n",
    "    -----------------\n",
    "    ts: time series data\n",
    "    \n",
    "    \n",
    "    output:\n",
    "    -----------------\n",
    "    trend[float] (slope)\n",
    "    '''\n",
    "    x = np.arange(0, len(ts))\n",
    "    x = sm.add_constant(x)\n",
    "    model = sm.OLS(ts, x).fit()\n",
    "    if model.pvalues['x1'] < 0.05:\n",
    "        return model.params['x1']\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "\n",
    "def get_mean_max_min_median_trend_sentiment(df):\n",
    "    '''\n",
    "    get the max value, min value, median value, trend value of all sentiment\n",
    "    \n",
    "    input:\n",
    "    -------------------------------\n",
    "    df[DataFrame]: input data containing sentiment data\n",
    "    \n",
    "    \n",
    "    output:\n",
    "    -------------------------------\n",
    "    max value, min value, median value, trend value of all sentiment\n",
    "    \n",
    "    '''\n",
    "    output = pd.DataFrame()\n",
    "    sentiment = df.loc[:, 'I1_lag0':'I10_lag47']\n",
    "    for i in tqdm(range(1, 11)):\n",
    "        col_name = \"I{}\".format(str(i))\n",
    "        start, end = col_name + \"_lag0\", col_name + \"_lag47\"\n",
    "        theme_data = sentiment.loc[:, start:end]\n",
    "        output[col_name+\"_max\"] = theme_data.max(axis=1)\n",
    "        output[col_name+\"_min\"] = theme_data.min(axis=1)\n",
    "        output[col_name+\"_mean\"] = theme_data.mean(axis=1)\n",
    "        output[col_name+\"_median\"] = theme_data.median(axis=1)\n",
    "        output[col_name+\"_trend\"] = theme_data.apply(get_trend_of_ts, axis=1)\n",
    "        output[col_name+\"_has_outlier\"] = (theme_data.std(axis=1) > 2*overall_std).astype(int)\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "# Get the max, min, median, mean and trend of sentiment\n",
    "overall_std = np.array(df.loc[:, \"I1_lag0\":\"I10_lag47\"]).flatten().std()\n",
    "\n",
    "sentiment = get_mean_max_min_median_trend_sentiment(df)\n",
    "# Concatenate X1-X5 with sentiment feature\n",
    "train = pd.concat([df.loc[:, 'X1':\"X5\"], sentiment], axis=1)\n",
    "\n",
    "# split data into trainning data and validation data\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(train, Y,  \n",
    "                                                      test_size=0.2)\n",
    "\n",
    "\n",
    "parameter_grid = {\"C\":[0.6, 0.7, 0.8, 0.9, 1]}\n",
    "simple_model = GridSearchCV(LogisticRegression(), param_grid=parameter_grid,\n",
    "                            n_jobs=-1)\n",
    "simple_model.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "print(classification_report(y_valid, simple_model.predict(x_valid)))\n",
    "print(accuracy_score(y_valid, simple_model.predict(x_valid)))\n",
    "print(log_loss(y_valid, simple_model.predict_proba(x_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 8, 'min_samples_leaf': 128}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.17      0.24       704\n",
      "           1       0.49      0.69      0.58       723\n",
      "           2       0.39      0.47      0.43       673\n",
      "\n",
      "    accuracy                           0.44      2100\n",
      "   macro avg       0.43      0.44      0.41      2100\n",
      "weighted avg       0.43      0.44      0.41      2100\n",
      "\n",
      "0.44333333333333336\n",
      "1.0294926637031416\n"
     ]
    }
   ],
   "source": [
    "parameter_grid = {\"max_depth\":[4, 8, 16, 20], \"min_samples_leaf\":[4,32,64,128]}\n",
    "simple_model = GridSearchCV(RandomForestClassifier(n_estimators=100), \n",
    "                            param_grid=parameter_grid,\n",
    "                            n_jobs=-1)\n",
    "simple_model.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "print(simple_model.best_params_)\n",
    "print(classification_report(y_valid, simple_model.predict(x_valid)))\n",
    "print(accuracy_score(y_valid, simple_model.predict(x_valid)))\n",
    "print(log_loss(y_valid, simple_model.predict_proba(x_valid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment, X1-X5 and cross comparison\n",
    "\n",
    "- Use X1-X5, \n",
    "- max, min, median, mean, trend of sentiment data, \n",
    "- cross comparison of X1-X5 (E.X. whether X1 > X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82091\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.31      0.36       627\n",
      "           1       0.53      0.69      0.60       760\n",
      "           2       0.46      0.40      0.43       713\n",
      "\n",
      "    accuracy                           0.48      2100\n",
      "   macro avg       0.47      0.47      0.46      2100\n",
      "weighted avg       0.47      0.48      0.47      2100\n",
      "\n",
      "0.48\n",
      "1.0292395875960232\n"
     ]
    }
   ],
   "source": [
    "def add_cross_comparison(df):\n",
    "    '''\n",
    "    compare Xi with Xj, if Xi > Xj, then the value of new column \"XiXj\" is 1, else 0 \n",
    "    '''\n",
    "    for i in range(1, 6):\n",
    "        for j in range(1, 6):\n",
    "            col1, col2 = \"X{}\".format(str(i)), \"X{}\".format(str(j))\n",
    "            df[col1+col2] = df[col1] > df[col2]\n",
    "    return df\n",
    "\n",
    "\n",
    "# Concatenate X1-X5 with sentiment feature\n",
    "df = pd.concat([df.loc[:, 'X1':\"X5\"], sentiment], axis=1)\n",
    "# Add cross comparison feature\n",
    "df = add_cross_comparison(df)\n",
    "\n",
    "# split data into trainning data and validation data\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(df, Y,  \n",
    "                                                      test_size=0.2)\n",
    "\n",
    "\n",
    "parameter_grid = {\"C\":[0.5, 0.6, 0.7, 0.8, 0.9, 1]}\n",
    "simple_model = GridSearchCV(LogisticRegression(), param_grid=parameter_grid,\n",
    "                            n_jobs=-1)\n",
    "simple_model.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "print(classification_report(y_valid, simple_model.predict(x_valid)))\n",
    "print(accuracy_score(y_valid, simple_model.predict(x_valid)))\n",
    "print(log_loss(y_valid, simple_model.predict_proba(x_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['X1', 'X2', 'X3', 'X4', 'X5'], dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns[df.columns.str.match('X.+')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 10/10 [01:06<00:00,  6.67s/it]\n",
      "100%|██████████████████████████████████████████| 10/10 [00:28<00:00,  2.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.39      0.40      1319\n",
      "           1       0.55      0.56      0.55      1481\n",
      "           2       0.43      0.43      0.43      1400\n",
      "\n",
      "    accuracy                           0.46      4200\n",
      "   macro avg       0.46      0.46      0.46      4200\n",
      "weighted avg       0.46      0.46      0.46      4200\n",
      "\n",
      "0.4645238095238095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 10/10 [00:28<00:00,  2.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1489834539733035\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "\n",
    "\n",
    "def transform(df):\n",
    "    '''\n",
    "    add max, min, median, mean, trend, if_has_outlier features to sentiment data\n",
    "    '''\n",
    "    sentiment = get_mean_max_min_median_trend_sentiment(df)\n",
    "    # Concatenate X1-X5 with sentiment feature\n",
    "    df = pd.concat([df.loc[:, 'X1':\"X5\"], sentiment], axis=1)\n",
    "    return df\n",
    "    \n",
    "    \n",
    "class SimpleSentimentModel(TransformerMixin, BaseEstimator):\n",
    "    '''\n",
    "    model that only trained on sentiment data\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "    \n",
    "    def transform(self, df):\n",
    "        sentiment_col_mask = df.columns.str.match(\"I.+\")\n",
    "        sentiment_col_name = df.columns[sentiment_col_mask]\n",
    "        sentiment_features = df[sentiment_col_name]\n",
    "        return sentiment_features\n",
    "    \n",
    "    def fit(self, df, y):\n",
    "        sentiment_features = self.transform(df)\n",
    "\n",
    "        parameter_grid = {\"max_depth\":[4, 8, 16, 20], \"min_samples_leaf\":[4,32,64,128]}\n",
    "        self.model = GridSearchCV(RandomForestClassifier(n_estimators=100), \n",
    "                                    param_grid=parameter_grid,\n",
    "                                    n_jobs=-1)\n",
    "        self.model.fit(sentiment_features, y)\n",
    "        \n",
    "    def predict(self, df):\n",
    "        sentiment_features = self.transform(df)\n",
    "        simple_sentiment_output = self.model.predict_proba(sentiment_features)\n",
    "        return simple_sentiment_output\n",
    "    \n",
    "    def fit_predict(self, df, y):\n",
    "        sentiment_features = self.transform(df)\n",
    "\n",
    "        parameter_grid = {\"max_depth\":[4, 8, 16, 20], \"min_samples_leaf\":[4,32,64,128]}\n",
    "        self.model = GridSearchCV(RandomForestClassifier(n_estimators=100), \n",
    "                                    param_grid=parameter_grid,\n",
    "                                    n_jobs=-1)\n",
    "        self.model.fit(sentiment_features, y)\n",
    "        return self.model.predict_proba(sentiment_features)\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "class SimpleReturnModel(TransformerMixin, BaseEstimator):\n",
    "    '''\n",
    "    model that only trained on return data\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "    \n",
    "    def transform(self, df):\n",
    "        return_col_name = df.columns[df.columns.str.match('X.+')]\n",
    "        return_features = df[return_col_name]\n",
    "        return return_features\n",
    "    \n",
    "    def fit(self, df, y):\n",
    "        return_features = self.transform(df)\n",
    "\n",
    "        parameter_grid = {\"C\":[0.6, 0.7, 0.8, 0.9, 1]}\n",
    "        self.model = GridSearchCV(LogisticRegression(), param_grid=parameter_grid,\n",
    "                                n_jobs=-1)\n",
    "        self.model.fit(return_features, y)\n",
    "        \n",
    "    def predict(self, df):\n",
    "        return_features = self.transform(df)\n",
    "        simple_return_output = self.model.predict_proba(return_features)\n",
    "        return simple_return_output\n",
    "    \n",
    "    def fit_predict(self, df, y):\n",
    "        return_features = self.transform(df)\n",
    "\n",
    "        parameter_grid = {\"max_depth\":[4, 8, 16, 20], \"min_samples_leaf\":[4,32,64,128]}\n",
    "        self.model = GridSearchCV(RandomForestClassifier(n_estimators=100), \n",
    "                                    param_grid=parameter_grid,\n",
    "                                    n_jobs=-1)\n",
    "        self.model.fit(return_features, y)\n",
    "        return self.model.predict_proba(return_features)\n",
    "    \n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "def stacking1(df, y):\n",
    "    '''\n",
    "    stack sentiment model and return model (failed!!!!!! doesn't give good performance)\n",
    "    '''\n",
    "    df = transform(df)\n",
    "    \n",
    "    parameter_grid = {\"max_depth\":[4, 8, 16, 20], \"min_samples_leaf\":[4,32,64,128]}\n",
    "    stack_model = GridSearchCV(RandomForestClassifier(n_estimators=100), \n",
    "                                param_grid=parameter_grid,\n",
    "                                n_jobs=-1)\n",
    "    stack_model.fit(x_train, y_train)\n",
    "    \n",
    "    sentiment_model = SimpleSentimentModel()\n",
    "    return_model = SimpleReturnModel()\n",
    "    \n",
    "    sentiment_output = sentiment_model.fit_predict(df, y)\n",
    "    return_output = return_model.fit_predict(df, y)\n",
    "    cross_features = sentiment_output*return_output\n",
    "    \n",
    "    second_layer = np.concatenate([return_output, sentiment_output, cross_features], axis=1)\n",
    "    \n",
    "    stack_model.fit(second_layer, y)\n",
    "   \n",
    "    def prediction_function(df):\n",
    "        df = transformation(df)\n",
    "        \n",
    "        sentiment_output = sentiment_model.predict(df)\n",
    "        return_output = return_model.predict(df)\n",
    "        cross_features = sentiment_output*return_output\n",
    "        \n",
    "        second_layer = np.concatenate([return_output, sentiment_output, cross_features], axis=1)\n",
    "\n",
    "        return stack_model.predict_proba(second_layer)\n",
    "        \n",
    "    return prediction_function, stack_model\n",
    "\n",
    "\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(df, Y,  \n",
    "                                                      test_size=0.3)\n",
    "    \n",
    "prediction_pipeline, final_lr_model = stacking(x_train, y_train)\n",
    "\n",
    "\n",
    "y_pred =  np.argmax(prediction_pipeline(x_valid), axis=1)\n",
    "print(classification_report(y_valid, y_pred))\n",
    "print(accuracy_score(y_valid,y_pred))\n",
    "print(log_loss(y_valid, prediction_pipeline(x_valid)))\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 10/10 [00:33<00:00,  3.36s/it]\n"
     ]
    }
   ],
   "source": [
    "test_filename = './input_data/input_test_PkjtqdQ.csv'\n",
    "test = pd.read_csv(test_filename).iloc[:, 1:]\n",
    "\n",
    "output = prediction_pipeline(test)\n",
    "output = pd.DataFrame(output, columns=['Target -1', 'Target 0', 'Target 1'])\n",
    "output.to_csv(\"./output_data/lr_sentiment_stack_model.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a>Output submission file into the desired format</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 10/10 [00:33<00:00,  3.35s/it]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_with_cross_comparison' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-083cd878f913>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# Predict probability of test data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msimple_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_with_cross_comparison\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Target -1'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Target 0'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Target 1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./output_data/lr_sentiment_decision_tree.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_with_cross_comparison' is not defined"
     ]
    }
   ],
   "source": [
    "# Read test file\n",
    "test_filename = './input_data/input_test_PkjtqdQ.csv'\n",
    "df = pd.read_csv(test_filename).iloc[:, 1:]\n",
    "\n",
    "# Get the max, min, median, mean and trend of sentiment\n",
    "sentiment = get_mean_max_min_median_trend_sentiment(df)\n",
    "df = pd.concat([df.loc[:, 'X1':\"X5\"], sentiment], axis=1)\n",
    "\n",
    "# add the cross comparison features\n",
    "# df_with_cross_comparison = add_cross_comparison(df)\n",
    "\n",
    "# Predict probability of test data\n",
    "output = simple_model.predict_proba(df)\n",
    "#output = simple_model.predict_proba(df_with_cross_comparison)\n",
    "output = pd.DataFrame(output, columns=['Target -1', 'Target 0', 'Target 1'])\n",
    "output.to_csv(\"./output_data/lr_sentiment_decision_tree.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
